
@inproceedings{maggi_fast_2011,
	location = {Melacca, Malaysia},
	title = {A fast eavesdropping attack against touchscreens},
	isbn = {978-1-4577-2155-7 978-1-4577-2154-0 978-1-4577-2153-3},
	url = {http://ieeexplore.ieee.org/document/6122840/},
	doi = {10.1109/ISIAS.2011.6122840},
	abstract = {The pervasiveness of mobile devices increases the risk of exposing sensitive information on the go. In this paper, we arise this concern by presenting an automatic attack against modern touchscreen keyboards. We demonstrate the attack against the Apple {iPhone}—2010’s most popular touchscreen device—although it can be adapted to other devices (e.g., Android) that employ similar key-magnifying keyboards. Our attack processes the stream of frames from a video camera (e.g., surveillance or portable camera) and recognizes keystrokes online, in a fraction of the time needed to perform the same task by direct observation or ofﬂine analysis of a recorded video, which can be unfeasible for large amount of data. Our attack detects, tracks, and rectiﬁes the target touchscreen, thus following the device or camera’s movements and eliminating possible perspective distortions and rotations In real-world settings, our attack can automatically recognize up to 97.07 percent of the keystrokes (91.03 on average), with 1.15 percent of errors (3.16 on average) at a speed ranging from 37 to 51 keystrokes per minute.},
	eventtitle = {2011 7th International Conference on Information Assurance and Security ({IAS})},
	pages = {320--325},
	booktitle = {2011 7th International Conference on Information Assurance and Security ({IAS})},
	publisher = {{IEEE}},
	author = {Maggi, Federico and Gasparini, Simone and Boracchi, Giacomo},
	urldate = {2024-10-28},
	date = {2011-12},
	langid = {english},
}

@inproceedings{mathis_virtual_2022,
	title = {Virtual Reality Observations: Using Virtual Reality to Augment Lab-Based Shoulder Surfing Research},
	url = {https://ieeexplore.ieee.org/abstract/document/9756826?casa_token=Bl87z6Yacx0AAAAA:GRrF3UxwNJZHu9gVGlYfYz3OZEIsoaZhwSXbjTHZ6aPfrKog3mK3KVFaB1VKl0HA-8n27q52},
	doi = {10.1109/VR51125.2022.00048},
	shorttitle = {Virtual Reality Observations},
	abstract = {Given the difficulties of studying the shoulder surfing resistance of authentication systems in a live setting, researchers often ask study participants to shoulder surf authentications by watching two-dimensional (2D) video recordings of a user authenticating. How-ever, these video recordings do not provide participants with a realistic shoulder surfing experience, creating uncertainty in the value and validity of lab-based shoulder surfing experiments. In this work, we exploit the unique characteristics of virtual reality ({VR}) and study the use of non-immersive/immersive {VR} recordings for shoulder surfing research. We conducted a user study (N=18) to explore the strengths and weaknesses of such a {VR}-based shoulder surfing research approach. Our results suggest that immersive {VR} observations result in a more realistic shoulder surfing experience, in a significantly higher sense of being part of the authentication environment, in a greater feeling of spatial presence, and in a higher level of involvement than 2D video observations without impacting participants’ observation performance. This suggests that studying shoulder surfing in {VR} is advantageous in many ways compared to currently used approaches, e.g., participants can freely choose their observation angle rather than being limited to a fixed observation angle as done in current methods. We discuss the strengths and weaknesses of using {VR} for shoulder surfing research and conclude with four recommendations to help researchers decide when (and when not) to employ {VR} for shoulder surfing research in the authentication research domain.},
	eventtitle = {2022 {IEEE} Conference on Virtual Reality and 3D User Interfaces ({VR})},
	pages = {291--300},
	booktitle = {2022 {IEEE} Conference on Virtual Reality and 3D User Interfaces ({VR})},
	author = {Mathis, Florian and O’Hagan, Joseph and Khamis, Mohamed and Vaniea, Kami},
	urldate = {2024-10-28},
	date = {2022-03},
	note = {{ISSN}: 2642-5254},
	keywords = {Authentication, Privacy, Resistance, Shoulder Surfing, Three-dimensional displays, Two dimensional displays, Uncertainty, Virtual Reality, Virtual reality},
}

@article{zhang_understanding_nodate,
	title = {Understanding People’s Privacy Attitudes Towards Video Analytics Technologies},
	abstract = {Cameras are everywhere, and are increasingly coupled with video analytics software that can identify our face, track our mood, recognize what we are doing and more. We present the results of a 10-day in situ study designed to understand how people feel about these capabilities, looking both at the extent to which they expect to encounter them at venues they visit as part of their everyday activities and at how comfortable they are with the presence of such technologies across a range of realistic scenarios. Results indicate that while some widespread deployments are expected by many (e.g. surveillance in public spaces), others are not, with some making people feel particularly uncomfortable. Our results further show that people’s privacy preferences and expectations are complicated and vary with a number of factors such as the purpose for which footage is captured and analyzed, the particular venue where it is captured, or who it is shared with. Finally, we consider recent technology enabling entities that deploy video analytics software to only apply it to people who provide consent (“opt in”) “at or before the point of collection,” as required under new data privacy laws like the General Data Protection Regulation ({GDPR}). Because obtaining consent from users at or before each point of collection could result in signifcant user burden, we use our data to explore the possibility of developing predictive models that could one day help users provide such consent, and discuss different possible confgurations of such functionality.},
	author = {Zhang, Shikun Aerin and Bauer, Lujo and Feng, Yuanyuan and Cranor, Lorrie},
	langid = {english},
}

@online{noauthor_data_nodate,
	title = {Data Protection Act 2018},
	url = {https://www.legislation.gov.uk/ukpga/2018/12/contents},
	urldate = {2024-10-27},
	note = {Publisher: King's Printer of Acts of Parliament},
}

@online{noauthor_data_nodate-1,
	title = {Data protection},
	url = {https://www.gov.uk/data-protection},
	abstract = {The Data Protection Act ({DPA}) controls how personal information can be used and your rights to ask for information about yourself},
	titleaddon = {{GOV}.{UK}},
	urldate = {2024-10-27},
	langid = {english},
}

@article{sun_privacymask_2023,
	title = {{PrivacyMask}: Real-world privacy protection in face {ID} systems},
	volume = {20},
	issn = {1551-0018},
	url = {http://www.aimspress.com/article/doi/10.3934/mbe.2023083},
	doi = {10.3934/mbe.2023083},
	shorttitle = {{PrivacyMask}},
	abstract = {Recent works have illustrated that many facial privacy protection methods are effective in specific face recognition algorithms. However, the {COVID}-19 pandemic has promoted the rapid innovation of face recognition algorithms for face occlusion, especially for the face wearing a mask. It is tricky to avoid being tracked by artificial intelligence only through ordinary props because many facial feature extractors can determine the {ID} only through a tiny local feature. Therefore, the ubiquitous high-precision camera makes privacy protection worrying. In this paper, we establish an attack method directed against liveness detection. A mask printed with a textured pattern is proposed, which can resist the face extractor optimized for face occlusion. We focus on studying the attack efficiency in adversarial patches mapping from two-dimensional to three-dimensional space. Specifically, we investigate a projection network for the mask structure. It can convert the patches to fit perfectly on the mask. Even if it is deformed, rotated and the lighting changes, it will reduce the recognition ability of the face extractor. The experimental results show that the proposed method can integrate multiple types of face recognition algorithms without significantly reducing the training performance. If we combine it with the static protection method, people can prevent face data from being collected.},
	pages = {1820--1840},
	number = {2},
	journaltitle = {Mathematical Biosciences and Engineering},
	shortjournal = {{MBE}},
	author = {Sun, Guangmin and Wang, Hao and Bai, Yu and Zheng, Kun and Zhang, Yanjun and Li, Xiaoyong and Liu, Jie},
	urldate = {2024-10-27},
	date = {2023},
	langid = {english},
}

@online{noauthor_just_nodate,
	title = {Just gaze and wave {\textbar} Proceedings of the 11th {ACM} Symposium on Eye Tracking Research \& Applications},
	url = {https://dl.acm.org/doi/abs/10.1145/3314111.3319837},
	urldate = {2024-10-27},
}

@online{noauthor_just_nodate-1,
	title = {Just gaze and wave},
	url = {https://dl.acm.org/doi/epdf/10.1145/3314111.3319837},
	urldate = {2024-10-27},
	langid = {english},
	doi = {10.1145/3314111.3319837},
}

@article{lian_smart_2013,
	title = {Smart privacy-preserving screen based on multiple sensor fusion},
	volume = {59},
	issn = {1558-4127},
	url = {https://ieeexplore.ieee.org/document/6490252?pq-origsite=primo_ra},
	doi = {10.1109/TCE.2013.6490252},
	abstract = {Privacy problems arise with the popular usage of personal devices with display screen, e.g., laptop, smart phone or pad, in public areas, one of which is the screen peeping. This paper proposes a smart privacy-preserving screen system that can detect someone else see the consumer's screen and then protect the screen automatically and adaptively. It depends on multiple sensors, i.e., video camera module, ultrasonic distance module, light sensor module, to detect screen peeping, user distance and environmental lightness, and decide whether to adjust the screen's lightness. With limited lightness or contrast, the screen can only be seen by the consumer, while others cannot watch the screen clearly. Especially, the screen peeping detection scheme is composed of several algorithms, i.e., eye detection, eye pair decision and person counting. Based on the video frames captured by the video camera module, the scheme will detect eyes in the video frame, decide the number of persons peeping at the private screen, and inform the smart screen to protect the screen by adjusting the light or contrast. With the smart phone as an example, various experiments are done and comparative results show that the proposed scheme obtains better performance than existing works and is a good solution for automatic screen privacy protection.},
	pages = {136--143},
	number = {1},
	journaltitle = {{IEEE} Transactions on Consumer Electronics},
	author = {Lian, Shiguo and Hu, Wei and Song, Xingguang and Liu, Zhaoxiang},
	urldate = {2024-10-27},
	date = {2013-02},
	note = {Conference Name: {IEEE} Transactions on Consumer Electronics},
	keywords = {Acoustics, Cameras, Films, Mobile communication, Privacy, Sensor fusion, Smart phones, eye detection, privacy preserving, sensor fusion, smart screen},
}

@article{bace_privacyscout_2022,
	title = {{PrivacyScout}: Assessing Vulnerability to Shoulder Surfing on Mobile Devices},
	volume = {2022},
	rights = {https://creativecommons.org/licenses/by-nc-nd/3.0/},
	issn = {2299-0984},
	url = {https://petsymposium.org/popets/2022/popets-2022-0090.php},
	doi = {10.56553/popets-2022-0090},
	shorttitle = {{PrivacyScout}},
	abstract = {One approach to mitigate shoulder surﬁng attacks on mobile devices is to detect the presence of a bystander using the phone’s front-facing camera. However, a person’s face in the camera’s ﬁeld of view does not always indicate an attack. To overcome this limitation, in a novel data collection study (N=16), we analysed the inﬂuence of three viewing angles and four distances on the success of shoulder surﬁng attacks. In contrast to prior works that mainly focused on user authentication, we investigated three common types of content susceptible to shoulder surﬁng: text, photos, and {PIN} authentications. We show that the vulnerability of text and photos depends on the observer’s location relative to the device, while {PIN} authentications are vulnerable independent of the observation location. We then present {PrivacyScout} – a novel method that predicts the shoulder-surﬁng risk based on visual features extracted from the observer’s face as captured by the front-facing camera. Finally, evaluations from our data collection study demonstrate our method’s feasibility to assess the risk of a shoulder surﬁng attack more accurately.},
	pages = {650--669},
	number = {3},
	journaltitle = {Proceedings on Privacy Enhancing Technologies},
	shortjournal = {{PoPETs}},
	author = {Bâce, Mihai and Saad, Alia and Khamis, Mohamed and Schneegass, Stefan and Bulling, Andreas},
	urldate = {2024-10-27},
	date = {2022-07},
	langid = {english},
}

@inproceedings{brudy_is_2014,
	location = {New York, {NY}, {USA}},
	title = {Is anyone looking? mediating shoulder surfing on public displays (the video)},
	isbn = {978-1-4503-2474-8},
	url = {https://doi.org/10.1145/2559206.2579528},
	doi = {10.1145/2559206.2579528},
	series = {{CHI} {EA} '14},
	shorttitle = {Is anyone looking?},
	abstract = {When a person interacts with a display in an open area, sensitive information becomes visible to shoulder-surfing passers-by. While a person's body shields small displays, shielding is less effective as display area increases. To mitigate this problem, we sense spatial relationships between the passerby, person and display. Awareness of onlookers is provided through visual cues: flashing screen borders, a 3D model mirroring the onlooker's position and gaze, and an indicator that illustrates their gaze direction. The person can react with a gesture that commands the display to black out personal windows, or to collect them on one side. Alternately, the display will automatically darken screen regions visible by the onlooker, but leaving the display area shielded by the person's body unaltered (thus allowing the person to continue their actions). The person can also invite the onlooker to collaborate with them via a gesture that reverses these protective mechanisms. This video illustrates these and other approaches to mitigate shoulder surfing.},
	pages = {159--160},
	booktitle = {{CHI} '14 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Brudy, Frederik and Ledo, David and Greenberg, Saul},
	urldate = {2024-10-27},
	date = {2014-04-26},
}

@online{noauthor_pdf_nodate,
	title = {({PDF}) Occupying Another’s Digital Space: Privacy of Smartphone Users as a Situated Practice},
	url = {https://www.researchgate.net/publication/378436670_Occupying_Another's_Digital_Space_Privacy_of_Smartphone_Users_as_a_Situated_Practice},
	urldate = {2024-10-27},
}

@online{noauthor_imagenet_nodate,
	title = {{ImageNet}},
	url = {https://www.image-net.org/},
	urldate = {2024-10-26},
}

@inproceedings{redmon_you_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {You Only Look Once: Unified, Real-Time Object Detection},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780460/},
	doi = {10.1109/CVPR.2016.91},
	shorttitle = {You Only Look Once},
	abstract = {We present {YOLO}, a new approach to object detection. Prior work on object detection repurposes classiﬁers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {779--788},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	urldate = {2024-10-26},
	date = {2016-06},
	langid = {english},
}

@inproceedings{eiband_understanding_2017,
	location = {Denver Colorado {USA}},
	title = {Understanding Shoulder Surfing in the Wild: Stories from Users and Observers},
	isbn = {978-1-4503-4655-9},
	url = {https://dl.acm.org/doi/10.1145/3025453.3025636},
	doi = {10.1145/3025453.3025636},
	shorttitle = {Understanding Shoulder Surfing in the Wild},
	abstract = {Research has brought forth a variety of authentication systems to mitigate observation attacks. However, there is little work about shoulder surﬁng situations in the real world. We present the results of a user survey (N=174) in which we investigate actual stories about shoulder surﬁng on mobile devices from both users and observers. Our analysis indicates that shoulder surﬁng mainly occurs in an opportunistic, nonmalicious way. It usually does not have serious consequences, but evokes negative feelings for both parties, resulting in a variety of coping strategies. Observed data was personal in most cases and ranged from information about interests and hobbies to login data and intimate details about third persons and relationships. Thus, our work contributes evidence for shoulder surﬁng in the real world and informs implications for the design of privacy protection mechanisms.},
	eventtitle = {{CHI} '17: {CHI} Conference on Human Factors in Computing Systems},
	pages = {4254--4265},
	booktitle = {Proceedings of the 2017 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Eiband, Malin and Khamis, Mohamed and Von Zezschwitz, Emanuel and Hussmann, Heinrich and Alt, Florian},
	urldate = {2024-10-26},
	date = {2017-05-02},
	langid = {english},
}

@article{co-supervisor_ispy_nodate,
	title = {{iSpy}: A Real-Time Application of Shoulder Surﬁng Attacks},
	author = {Co-Supervisor, Thesis and Zanero, Stefano},
	langid = {english},
}

@inproceedings{pearson_cant_2022,
	location = {New Orleans {LA} {USA}},
	title = {Can’t Touch This: Rethinking Public Technology in a {COVID}-19 Era},
	isbn = {978-1-4503-9157-3},
	url = {https://dl.acm.org/doi/10.1145/3491102.3501980},
	doi = {10.1145/3491102.3501980},
	shorttitle = {Can’t Touch This},
	eventtitle = {{CHI} '22: {CHI} Conference on Human Factors in Computing Systems},
	pages = {1--14},
	booktitle = {{CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Pearson, Jennifer and Bailey, Gavin and Robinson, Simon and Jones, Matt and Owen, Tom and Zhang, Chi and Reitmaier, Thomas and Steer, Cameron and Carter, Anna and Sahoo, Deepak Ranjan and Raju, Dani Kalarikalayil},
	urldate = {2024-10-09},
	date = {2022-04-29},
	langid = {english},
}

@inproceedings{abdrabou_understanding_2022,
	location = {New York, {NY}, {USA}},
	title = {Understanding Shoulder Surfer Behavior and Attack Patterns Using Virtual Reality},
	isbn = {978-1-4503-9719-3},
	url = {https://dl.acm.org/doi/10.1145/3531073.3531106},
	doi = {10.1145/3531073.3531106},
	series = {{AVI} '22},
	abstract = {In this work, we explore attacker behavior during shoulder surfing. As such behavior is often opportunistic and difficult to observe in real world settings, we leverage the capabilities of virtual reality\&nbsp;({VR}). We recruited 24 participants and observed their behavior in two virtual waiting scenarios: at a bus stop and in an open office space. In both scenarios, participants shoulder surfed private screens displaying different types of content. From the results we derive an understanding of factors influencing shoulder surfing behavior, reveal common attack patterns, and sketch a behavioral shoulder surfing model. Our work suggests directions for future research on shoulder surfing and can serve as a basis for creating novel approaches to mitigate shoulder surfing.},
	pages = {1--9},
	booktitle = {Proceedings of the 2022 International Conference on Advanced Visual Interfaces},
	publisher = {Association for Computing Machinery},
	author = {Abdrabou, Yasmeen and Rivu, Sheikh Radiah and Ammar, Tarek and Liebers, Jonathan and Saad, Alia and Liebers, Carina and Gruenefeld, Uwe and Knierim, Pascal and Khamis, Mohamed and Makela, Ville and Schneegass, Stefan and Alt, Florian},
	urldate = {2024-10-08},
	date = {2022-06-06},
}
