\documentclass[12pt]{article}

% Sets document language to English (some british conventions in
% hyphenation). Can also handle multilingual documents.
\usepackage[british]{babel}
\usepackage{csquotes}
\usepackage{pdflscape} 
\usepackage{geometry}
\usepackage{longtable}
\usepackage{gensymb}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{listings}
\lstset{
  tabsize=2,
  breaklines=true,
  captionpos=b,
  extendedchars=true,
  numbers=left,
  basicstyle=\ttfamily,
  commentstyle=\color{red!70!black},
  keywordstyle=\color{green!70!black},
  numberstyle=\tiny\color{black!50},
  stringstyle=\ttfamily\color{blue!50!black},
  backgroundcolor=\color{yellow!10}
}

% Uses the newer biblatex (with biber as backend) for citations and
% references. Can deal with non-ascii letters in author names.
\usepackage{biblatex}
\addbibresource{references.bib}

% Provides more maths support and the theorem environments.
\usepackage{amsmath}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Font handling here is intended for LuaTeX or XeTeX engine.
% Sets the font. In this case a font similar to Times.
\usepackage{fontspec}
\setmainfont{TeX Gyre Termes}
\setmonofont{Source Code Pro}[Scale=MatchLowercase]
% Unicode math fonts
\usepackage{unicode-math}
\setmathfont{texgyretermes-math.otf}

% Some generally useful packages:
% Provides \includegraphics to insert images.
\usepackage{graphicx}
% Provides \url to insert url links.
\usepackage{url}
% Provides colour support.
\usepackage{xcolor}
% Provides tables that are aesthetically more pleasing.
\usepackage{booktabs}
% Provides more configurable itemised and enumerated lists.
\usepackage{enumitem}
% Provides environment for defining vector graphics drawings.
\usepackage{tikz}
% Provides environment for code listings.
\usepackage{listings}
\lstset{
  tabsize=2,
  breaklines=true,
  captionpos=b,
  extendedchars=true,
  numbers=left,
  basicstyle=\ttfamily,
  commentstyle=\color{red!70!black},
  keywordstyle=\color{green!70!black},
  numberstyle=\tiny\color{black!50},
  stringstyle=\ttfamily\color{blue!50!black},
  backgroundcolor=\color{yellow!10}
}
% Provides hyperlinks in the pdf. Not suitable for printed documents,
% but fine here.
\usepackage[pdfborder={0 0 0},colorlinks=true,allcolors={blue!40!black}]{hyperref}

% Load the style file (title page and declarations) for the document.
\usepackage[]{swanseaTitleUG}

% Paragraphs are typeset with a small skip between them.
\usepackage[parfill]{parskip} 

% User supplied information that appears on title page. Do edit these!
\title{ Shoulder Surfing Detection}
\author{Katie Peacey}
\studentid{2214646}
\project{Shoulder Surfing Detector}

% Table of contents only lists 2 levels.
\setcounter{tocdepth}{2}

\begin{document}
\pagenumbering{roman}

\maketitle
\studentdeclarations

\begin{frontmatterparagraph}{Abstract}
Shoulder surfing is the act of visually observing someone’s private screen interactions without their consent and remains a significant privacy concern in public and semi-public spaces. From entering passwords at ATMs to composing confidential emails on laptops in cafes, users are increasingly vulnerable to visual eavesdropping. Past work on solutions such as physical privacy screens or behavioural adjustments often require users to alter their behaviour or purchase additional hardware, however these approaches frequently compromise usability, reduce screen visibility or demand constant user vigilance. This paper introduces a software-based shoulder surfing detection system developed using Python. The system uses real-time gaze tracking to identify the presence of unauthorised viewers and responds by sounding an alert and dimming the display to obscure sensitive content. The system has been designed  with the user experience in mind, so seeks to be non-intrusive and responsive to the user context indicating a strong potential for broader application across both mobile and desktop platforms.
\end{frontmatterparagraph}

% Build the table of contents page.
\tableofcontents

% These lists are optional, especially if they are empty.
\listoffigures
\listoftables
\clearpage

% Reset numeric page numbering from page 1
\pagenumbering{arabic}

\section{Introduction}
\label{sec:intro}

As mobile computing becomes more embedded in daily life, individuals are more likely to interact with laptops, tablets, and smartphones in public or semi-public spaces such as cafes, libraries, offices, or public transport. This exposure increases security and privacy concerns including the risk of shoulder surfing, which is the act of observing someone else’s screen to gather privacy or sensitive information without their consent.

Shoulder surfing can occur both deliberately and inadvertently. In its deliberate form, an attack may position themselves behind or beside a victim to watch as they enter passwords, read sensitive emails, or access personal data. Inadvertent cases can involve passers-by or nearby users unintentionally glancing at a screen due to curiosity or proximity, still resulting in an invasion of privacy. Eiband et al. describes shoulder surfing as both a common and underestimated threat, particularly in environments where users are less aware of their surroundings \cite{eiband_understanding_2017}. Similarly, research by Aviv et al. demonstrates how observational attacks can be used to accurately extract PINs, swipe patterns, and alphanumeric passwords with minimal effort \cite{aviv_towards_2017}.

The risk associated with shoulder surfing is not limited to password theft as attackers can also capture financial details, health records, business correspondence or even biometric data by simply watching a screen. As these attacks often require no technical skill and can be carried out using the naked eye or a camera, their easy method of attack makes them especially concerning \cite{block_impact_2010}. In addition, modern device screens with high brightness and wide viewing angles inadvertently help attackers by making the screen contents visible from wider perspectives.

Given these risks, mitigating shoulder surfing has become a growing concern within security. Past solutions, such as privacy screen protectors or alternative authentication methods like Passface \cite{liu_passface_2021} and Draw-a-Secret (DAS) \cite{nali_analyzing_2004}, have shown some success but often rely on hardware or suffer from poor long-term usability. These approaches can interrupt normal user behaviour or degrade the user experience allowing for an interest in a protection method that can integrate seamlessly into users’ everyday interactions.

This paper introduces a gaze-based detection system as a productive solution to shoulder surfing. Implemented in Python, the system utilizes computer vision and gaze estimation techniques to monitor the presence of faces within a device’s camera view. When more than one face is detected and the user’s gaze is focused on the screen, the system triggers a subtle alert, such as screen dimming or an audio cue, to warn the user of a potential privacy threat.

The primary aim of this project is to develop a lightweight, software-only solution that identifies shoulder surfing attempts in real-time without interrupting the user’s workflow. To achieve this, the system is designed to be unobtrusive, user-friendly, and adaptable across various public or shared environments. The objectives include:

\begin{itemize}
  \item Implementing face detection and gaze tracking techniques suitable for real-time use.
  \item Designing a responsive user interface that allows users to customise detection sensitivity and feedback settings.
  \item Evaluating the system’s accuracy, usability, and effectiveness through controlled user studies.
  \item Iteratively refining the system based on user feedback and performance data.
\end{itemize}

These goals reflect a focus on practical usability as well as technical reliability, positioning the system at the intersection of computer vision, human-computer interaction, and privacy-aware design. This paper presents a review of related work, details the design and implementation of the system, and evaluates its performance and user experience through a structured study to offer a novel approach to mitigate shoulder surfing attacks.

\section{Related Work}
\label{sec:Related Work} 

Shoulder surfing is an increasingly prevalent issue. The occurrence of shoulder surfing is not limited to malicious attacks, it also includes unintentional glances and peeking out of interest. Research by Eiband et al. (2017) demonstrates that shoulder-surfing is commonplace, especially on public transportation and in communal environments. Their study suggests that users often lack awareness of being observed and generally underestimate the risk. Similarly, studies by Aviv et al. (2017) and De Luca et al. (2014) show how attackers can retrieve PINs and passwords through observational techniques, emphasising the urgency of practical mitigation strategies.

The earliest responses to shoulder surfing have been physical: privacy filters and screen covers (Lian et al. 2013). These accessories reduced the screen’s visibility angle to deter side glances from others. However, they also reduce usability for the primary user, especially in shared or collaborative settings. Something more about physical defences... 

Something about software-based defences. For example, Wiedenbeck et al. (2005) evaluated the DAS (Draw-a-secret) approach, while Tari et al. (2006) explored PIN-entry systems that reduce predictability. Though promising, these methods often compromise efficiency and are reflected by users for being cumbersome (Kroeze & Olivier, 2008).

Computer vision techniques have gained traction for dynamic user-aware defences. Brudy et al. (2014) introduced methods to detect onlookers in the context of public displays, while Khamis et al. (2016) explored gaze-based user interfaces that adapt to attention shifts. Real-time face detection, such as YOLO (You-Only-Look-Once)(Redmon et al. 2016), provides a fast and accurate way to identify multiple individuals in a frame. Insert YOLO facts here. This will be further discussed.

Gaze interaction has been increasingly used not only for input but also for context awareness and security. Holland and Komogortsev (2012) demonstrated that haze can be used for continuous authentication, while Kumar et al. (2007) showed gaze patterns can distinguish between users. Recent work by Liu et al. (2007) investigates how gaze cues can signal environmental risks without interrupting primary tasks. Such ambient strategies align closely with Weiser and Brown’s (1996) concept of calm technology interfaces that remain in the background until needed.

Something about UX. Research by Cranor and Garfinkel (2005) and more recent works by Nappa et al. (2019) confirm that users resist tools that are overly intrusive or complex. This proves that the most effective solutions are those that operate passively, offer user control and integrate smoothly into daily workflows.

Our system stands in the intersection of these lines of research. It bridges the high-performance detection with subtle, user-oriented alerts, guided by formative research in UX and informed by the limitations of past solutions.

\section{System Design}
\label{sec:system design}

\subsection{Overview of the system}
\label{sec:Overview of the system}

The proposed solution is a software-based approach that detects potential shoulder surfing by using gaze tracking. It was developed in Python with a focus of usability, responsiveness and adaptability within different environments. The solution aims to minimise user disruption by only providing feedback when a potential threat is detected.

The system uses a live video input combined with a gaze tracking algorithm to determine whether a user is being observed. The central idea is to detect multiple faces within the webcam view and compare this to the gaze direction to decide whether an unauthorised person is looking at the user’s screen. Once detected, the system triggers a response where the screen brightness is reduced, and an audio alert plays to notify the user discreetly. The user can remain in control of the system via a graphical interface with start/stop buttons and customizable settings.

\subsubsection{Human-Centred Design and UX Principles}

The system was designed following established Human-Computer Interaction (HCI) principles, with particular reference to Nielsen’s usability heuristics and Don Norman’s principles of design. Rather than overwhelming users with intrusive pop-ups or constant alerts, the system alights with Norman’s idea of making important system state visible and providing natural mappings between intentions and outcomes. We ensured that feedback is delivered through screen dimming and subtle sounds which allow for clear communication without demanding excessive attention.

The interface supports discoverability and feedback as per Norman’s principles by allowing adjustable settings that can be easily understood and controlled. The settings allow for the screen brightness when dimmed to be altered using a sliding bar and alert sounds can be toggled on and off. We have also ensured that the layout is simple by using minimal buttons, clear labels and consistent interaction patterns so that the user can easily use the system through recognition an have a minimal chance of making errors (Neiman, 2019).

\subsubsection{Privacy and Local Processing}

The system was built with privacy by design in mind, so all data processing occurs locally on the user’s device and no images or videos are saved, transmitted, or stored. This approach aligns with GDPR principles and reassures users that their data remains under their control. Since the webcam feed never leaves the device and is not stored in any form, the system supports high standards of personal data protection.

\subsection{Architecture}
\label{Architecture}
The software works by capturing live video input using the OpenCV library, which provides an efficient way to access real-time video streams. Rather than storing video feeds, frames are processing in real time to identify potential privacy threats.

For face detection, the system makes use of the dlib library’s Histogram of Oriented Gradients (HOG)-based face detector where detection speed and accuracy are computationally efficient to identify presence of more than one person. Upon detecting a face, the system applies dlib’s 68-point facial landmark model to identify specific points around the eyes needed for gaze estimation. 

Using the key eye landmarks, the system estimates gaze direction through geometric calculations where the relative position of the pupil within the eye socket and alignment with eye corners is used to infer whether the user is focused on the screen. Detection logic can then flag a potential shoulder surfing threat when multiple faces are present, and the user’s gaze is focused on the screen.

When a threat is detected, the system initiates feedback to the user by dimming the screen using the screen\textunderscore brightness\textunderscore control library and plays an alert sound to attract the user’s attention. The user interface has been built with customtkinter which provides an interactive control panel that includes start and stop buttons, a live video preview and a log of recent detection events for transparency and usability. 

In the initial prototype, YOLO (You Only Look Once) was used for person detection due to its high-speed object recognition, however in further development, this was replaced with dlib to enable integrated facial landmark tracking for gaze estimation. Justification for this is discussed further in the paper.

Figure~\ref{fig:architecture} demonstrates the flow of the system and how the architecture has been built in a modular way to allow for improvements and further research.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.3\linewidth]{img/architecture.png}
    \caption{Diagram to show the overall system architecture flow of the proposed solution.}
    \label{fig:architecture}
\end{figure}


\subsubsection{YOLO (You Only Look Once)}
During early development of this project, an object detection algorithm called YOLO \textcolor{red}{ref here} was identified as a suitable method for detecting the presence of onlookers.
YOLO (You Only Look Once) is an object detection algorithm produced in 2015 by Redmon et al. which uses bounding boxes to predict the probability of what each object is \cite{redmon_you_2016}. The algorithm has been trained using the ImageNet dataset \cite{noauthor_imagenet_nodate} and can detect from traffic lights to toilets. The YOLO model splits each image into a grid and produces bounding boxes for each cell, a class probability is listed, and a confidence score is calculated. The class probability is the likelihood that each object belongs to each class (e.g. bottle, person, chair) and the confidence is the likelihood that there is an object in that cell \cite{redmon_you_2016}. An overall confidence score can then be calculated and produced to the user using the formula: 
\[
  Confidence = Object Confidence Score \times Class Probability
\]
The confidence will then be listed between 1 and 0, 0 being no object found.

During initial planning, the algorithm was imported into Python and connected to the device’s camera. It was verified that YOLO processes images at 45 frames per second \cite{redmon_you_2016} meaning a passer-by should be detected in the time they walk past the screen. Figure~\ref{fig:yolo} shows the code being run on an Apple MacBook (720p FaceTime HD camera) to demonstrate its ability to detect more than one person at different distances and angles.

Images \ref{fig:a} and \ref{fig:b} demonstrate how more than one person can be identified at once – something that the shoulder surfing detector needs to do. Facial features do not need to be present to be identified correctly. Image \ref{fig:c} shows the main user with the onlooker stood approximately 6 meters behind. This resulted in an average confidence score of 0.95 meaning it was a strong identification.

However, we discovered after user studies and testing that YOLO's limitation lies in its inability to infer gaze or user intent. It identifies 'who' is present but not 'what' they are doing, leading to the exploration of gaze tracking solutions.

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/fig1-img1.png}
         \caption{Forward facing}
         \label{fig:a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/fig1-img2.png}
         \caption{Backward facing}
         \label{fig:b}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/fig1-img3.png}
         \caption{From a distance}
         \label{fig:c}
     \end{subfigure}
        \caption{YOLO being used to detect more than one person in-front of the camera. The observer stands at different distances and angles to test the system.}
        \label{fig:yolo}
\end{figure}

\subsubsection{Eye detection using dlib}
To enable gaze-based detection, dlib was identified as an appropriate solution for its efficient face detection and facial landmark prediction capabilities. Dlib is an open-source software library (King, 2019) including 68-point facial landmark detection model (see figure \ref{fig:facial-landmarks}) that is based on the Histogram of Oriented Gradients (HOG) combined with a linear support vector machine (SVM) which allows for effective detection of faces within the camera (Kazemi & Sullivan, 2014).

The shape predictor in dlib uses multiple regression trees to detect facial landmarks including the eyes, nose and mouth. The model was identified as useful for this project due the accurate feature recognition of multiple people at once allowing for gaze estimations to be calculated.

Before implementation, the model was imported into Python and used to identify any faces with the camera. Figure~\ref{fig:dlib} shows the code being run on an Apple Macbook (720p FaceTime HD camera) to demonstrate how more than one person can be identify when facing forward.

Image \ref{fig:face_a} shows how more than one face can be identified at once where all 68 points are drawn onto the user. Images \ref{fig:face_b} and \ref{fig:face_c} demonstrate how a user cannot be identified when turned around or from approximately 3 meters behind. This highlights that the onlooker would need to present their facial features closely to the camera in order to be identified. Despite these challenged, dlib was identified as computationally efficient and reliable making it a practical choice for gaze detection.


\begin{figure}[h!]
\centering
\fbox{\includegraphics[width=.3\textwidth]{img/facial-landmarks.png}\hfill}

\caption{68 facial landmarks model as part of the dlib library \cite{noauthor_dlib_nodate}. }
\label{fig:facial-landmarks}
\end{figure}

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/face_img1.png}
         \caption{Forward facing}
         \label{fig:face_a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/face_img2.png}
         \caption{Backward facing}
         \label{fig:face_b}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/face_img3.png}
         \caption{From a distance}
         \label{fig:face_c}
     \end{subfigure}
        \caption{dlib facial recognition being used to detect more than one person in-front of the camera. The observer stands at different distances and angles to test the system.}
        \label{fig:dlib}
\end{figure}

Together, these models formed the core detection strategies used during prototyping and producing the system. Whilst YOLO was used during the first iteration to broadly identify the presence of more than one person, the dlib library was used during the final solution to accurately identify and track the gaze direction of a potential shoulder surfer. The direct use of these will be discussed during the implementation part of this paper.

\section{Implementation}
\subsection{Overview}

The implementation of the shoulder surfing detection system was produced using a user-centred, iterative development process. The system was built using Python due to its accessibility and wide library support \textcolor{red}{ref here}. The main libraries used include OpenCV \textcolor{red}{ref here} for real-time webcam capture, dlib \textcolor{red}{ref here} for face and landmark detection, and customtkinter \textcolor{red}{ref here} for building the graphical user interface. Testing and development were conducted on a Macbook with a 720p FaceTime HD webcam, although the software was designed to operate cross-platform where possible.
The development was structured into two major phases where the initial prototype relied solely on persona detection using YOLO, while the final version incorporated gaze estimation using dlib’s facial landmark detection. This section describes each phase, the system’s interface and the decisions made to support modularity and responsiveness.

\subsection{Prototype 1: YOLO-Based Detection}

The first version featured a basic graphical interface, consisting only of a start and stop controls hardcoded into the script. These were no adjustable settings or real-time feedback provided to the user. While this minimal interface was sufficient for functional testing, it lacked the usability features required for regular, user-friendly operation.

In this version, the YOLOv8 model for person detection was used due to its ability to detect multiple people at high speed (up to 45 FPS) making is a strong algorithm to help identify the presence of potential shoulder surfers in real time. The model was integrated into the system through the Ultralytics Python interface \textcolor{red}{ref here} and connected to the webcam feed via OpenCV.

The initial logic flagged a potential threat if more than one person was detected within the frame. If this condition was met, the system dimmed the screen and played an audio warning using the playsound library. It was clear from user studies (see \ref{sec:Evaluation}) that while the prototype was effective in detecting people at varying distances and angles, it lacked the ability to recognise whether the user was actually being observe causing false positives especially in busier environments.

\subsection{Prototype 2: Gaze Detection with dlib}

In response to the limitations to the first prototype, the second iteration introduced gaze tracking using dlib’s 68-point facial landmark model. This logic provided a lightweight and efficient method for frontal face detection where once a face was detected, specific landmarks around the eyes were used to estimate gaze direction.

Gaze was approximated by comparing the horizontal and vertical alignment of the pupil relative to the eye corners (see 4.3.1). A custom function determined whether the gaze was focused toward the centre, left or right which allowed the system to infer whether the user was likely looking at the screen. A shoulder surfing threat was only triggered when more than one face was present and the user’s gaze was focused on the screen, significantly reducing false positives from before.

This prototype also introduced a frame counter and delay which means that if a previously detected threat disappeared for a set number of frames, the system automatically restored brightness, improving responsiveness without introducing any major visual changes.

Alongside the improved detection logic, this iteration also introduced a new user interface to address the usability limitation of prototype 1. The updated GUI, discussed in the next section, provided real-time control, visual feedback and configuration options, making the system more user-friendly and practical for everyday use.

\subsubsection{Gaze Calculations}

To estimate gaze direction, the system computes midpoints and defines horizontal and vertical eye reference lines using facial landmarks from dlib's 68-point model. For the right eye:

\begin{itemize}

    \item Let \( P_{43} = (x_{43}, y_{43}) \) and \( P_{46} = (x_{46}, y_{46}) \) denote the outer and inner corners of the right eye.

    \item Let \( P_{44}, P_{45}, P_{48}, P_{47} \) denote the upper and lower eyelid landmarks.

\end{itemize}
The horizontal reference line is defined as:
\[
\text{Horizontal Line}_{\text{right}} = \overline{P_{43}P_{46}}
\]

The vertical reference line uses the midpoint of the upper and lower eyelids:
\[
M_{\text{top}} = \left( \frac{x_{44} + x_{45}}{2}, \frac{y_{44} + y_{45}}{2} \right)
\]

\[
M_{\text{bottom}} = \left( \frac{x_{48} + x_{47}}{2}, \frac{y_{48} + y_{47}}{2} \right)
\]

\[
\text{Vertical Line}_{\text{right}} = \overline{M_{\text{top}}M_{\text{bottom}}}
\]

This calculation is mirrored for the left eye using landmarks \( P_{37} \) through \( P_{42} \). A visal graphic is shown in figure \ref{fig:eye_calc} to help understand where the points are located in real life. In the system, these lines are visualised on the video frame to assist with approximate gaze direction detection. A code snippet of these calculations put into practice are provided in appendix \ref{app:gaze_calcs}.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{img/Eye_calc.jpg}
    \caption{Graphic to show key eye point landmark calculations taken from the 68-point landmarks used to detect and track gaze direction in the system.}
    \label{fig:eye_calc}
\end{figure}

\subsection{Graphical User Interface (GUI)}

The GUI (see figure \ref{fig:gui_overview}) was developed using the customtkinter library to offer a modern interface to the user. The sidebar layout was designed to give users easy access to core functions: starting and stopping detection, viewing logs, adjusting dimming brightness and toggling audio alerts.

Controls include:
\begin{itemize}
  \item Start/Stop buttons: Controls the detection, buttons change colour when active.
  \item Brightness slider: Adjusts screen dimming level.
  \item Sound toggle: Enables or disables warning sounds.
  \item Log panel: Displays recent events for transparency and trust.
\end{itemize}

The layout adheres to Don Norman’s usability principles by providing clear feedback, discoverability, and user control. Default settings were chosen for general usability but are customisable for different environments.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{img/gui.png}
    \caption{System interface with live feed}
    \label{fig:gui_face}
  \end{subfigure}
  \hfill
  \begin{subfigure}[c]{0.45\textwidth}
    \centering
    \includegraphics[width=0.6\linewidth]{img/sidebar.png}
    \caption{System control panel}
    \label{fig:gui_controls}
  \end{subfigure}
  \caption{Interface elements showing (a) the detection system in use, and (b) the customisable control panel.}
  \label{fig:gui_overview}
\end{figure}

\subsection{Technical Challenges and Solutions}

Several challenges arose during development:
\begin{itemize}
  \item Brightness control across platforms: On macOS, Python’s screen\textunderscore brightness\textunderscore control library had limited compatibility, so fallback shell commands were used.
  \item False positives in early testing: Without gaze logic, many non-threats triggered alerts. The second prototype addressed this by coupling gaze and presence detection.
  \item Low-light performance: Face and eye detection reduced in dim lighting. Histogram equalisation and threshold tuning were explored to enhance reliability.
  \item Real-time performance: To reduce lag for prototype 1, only every fifth video frame was processed during detection. This reduced CPU load while maintaining responsiveness.
\end{itemize}

These technical improvements ensured the system remained responsive, lightweight, and practical for everyday use. The implementation phase demonstrated the feasibility of real-time, ambient shoulder surfing detection using accessible, open-source tools.

\section{Evaluation}

\subsection{Overview of Evaluation Approach}

To assess the effectiveness and usability of the shoulder surfing detection system, a user study was conducted. The aim was to evaluate both the technical performance of the detection algorithm and the user experience when interacting with the system in realistic scenarios. The study focused on identifying false positives, false negatives, and overall detection accuracy, while also capturing participant perceptions of usability, intrusiveness, and effectiveness.

The evaluation followed a two-stage approach. The first involved testing the initial prototype, which relied solely on YOLO-based person detection, while the second focused on the refined system that integrated gaze tracking using dlib. Feedback and performance results from the first stage directly informed improvements made in the second prototype.

\subsection{Study Design}
The study included 16 participants with varying levels of technical expertise. Each participant was asked to complete a series of tasks involving the entry of sensitive information (e.g., email, PIN, passwords) on a laptop configured with the detection system. Scenarios were designed to reflect different real-world conditions, including variations in lighting (bright, dim, dark) and observer angles (behind, side, diagonal).

A confederate was positioned to simulate shoulder surfing in pre-determined scenarios, while participants were informed that potential observers might appear during the study. System responses—such as whether an alert was triggered and whether the screen dimmed—were recorded along with system logs tracking detection events.

\subsection{Evaluation Metrics}
To evaluate system performance, the following metrics were used:

Detection Accuracy: Number of correctly identified shoulder surfing events versus the total number of actual events.

False Positives: Incidents where the system incorrectly flagged a threat.

False Negatives: Missed detection of an actual shoulder surfing attempt.

Usability Ratings: Participant ratings on ease of use, intrusiveness, and perceived effectiveness.

\subsection{Results}
Results from Prototype 1 revealed several limitations. While YOLO reliably detected the presence of additional individuals in the frame, it often triggered alerts even when observers were not looking at the screen. This led to a relatively high rate of false positives, particularly in crowded environments or when multiple people passed behind the user. Detection accuracy was moderate, and usability ratings were mixed, with some participants finding the system unnecessarily disruptive.

In contrast, the refined prototype incorporating gaze tracking significantly reduced false positives. By combining presence detection with eye-tracking logic, the system only triggered alerts when the user's gaze was confirmed on the screen and another person was present. This reduced unnecessary interruptions and improved user trust. Participants reported a higher sense of security and appreciated the ambient nature of the alerts (screen dimming and soft audio cues).

Quantitatively, the final system achieved a notable increase in detection accuracy, with most participants agreeing that the system alerted them appropriately during simulated attacks. False negatives were rare, though a few instances occurred under poor lighting conditions or extreme viewing angles. The usability feedback was largely positive: participants described the interface as easy to use, and most reported that the system did not interfere with their tasks.

\subsection{Post-Study Survey Findings}
The post-study survey collected both quantitative and qualitative feedback. Most participants rated the system highly in terms of effectiveness, with over two-thirds indicating they would use a similar system in real-life scenarios. Users appreciated the passive design and found the visual dimming to be less distracting than auditory alerts alone.

When asked about improvements, common suggestions included greater control over alert sensitivity and the option to disable audio cues in quiet environments. Some participants also requested visual feedback showing why an alert was triggered—such as an icon or message confirming detection.

\subsection{Analysis and Interpretation}
The results indicate that combining presence detection with gaze estimation provides a more robust and user-friendly approach to shoulder surfing prevention. The key insight is that context-aware logic—evaluating both the environment and the user's behaviour—can substantially reduce unnecessary system activations. The modularity of the design also allows for future integration of more advanced gaze tracking methods or environmental sensors.

Participants valued the system most in public or semi-public spaces, where awareness of onlookers is often reduced. Although detection performance declined slightly under low lighting conditions, the core functionality remained reliable across most test scenarios.

\subsection{Limitations}
The evaluation study was limited by the sample size and the artificial nature of the testing environment. While every effort was made to simulate realistic use, participants may have behaved differently knowing they were being observed. The webcam hardware also restricted the field of view and resolution, potentially affecting detection at wider angles or greater distances.

Additionally, the study primarily focused on short-term interaction. Future evaluations should explore long-term use, user fatigue, and adaptation to ambient feedback over extended periods.

\subsection{Summary}
The evaluation demonstrates that the final prototype successfully addressed many of the weaknesses found in the initial design. By incorporating gaze awareness and refining the detection logic, the system achieved both higher accuracy and improved user satisfaction. These results support the viability of real-time, software-only shoulder surfing detection systems in enhancing user privacy without compromising usability.

\printbibliography 

\clearpage\appendix

\section{Calculations for gaze tracking}
\label{app:gaze_calcs}

\lstinputlisting[language=Python, caption={Calculations used for gaze tracking in Python.}, firstline=188, lastline=205]{./listings/Stats.py}

\end{document}
